[
    {
        "id": "pushkin-poetry",
        "title": "Pushkin's Poetry: The First Markov Chain",
        "description": "Recreate Andrey Markov's 1906 analysis of vowel-consonant patterns in Pushkin's 'Eugene Onegin'—the very first application of Markov chains!",
        "category": "classic",
        "difficulty": "beginner",
        "applications": [
            "Literary Analysis",
            "Historical Mathematics",
            "Pattern Recognition"
        ],
        "interactiveDemo": true,
        "design": {
            "states": [
                {
                    "id": "vowel",
                    "name": "Vowel",
                    "x": 700,
                    "y": 650,
                    "color": "#fbbf24"
                },
                {
                    "id": "consonant",
                    "name": "Consonant",
                    "x": 1300,
                    "y": 650,
                    "color": "#60a5fa"
                }
            ],
            "transitions": [
                {
                    "id": "vowel-vowel",
                    "from": "vowel",
                    "to": "vowel",
                    "probability": 0.125
                },
                {
                    "id": "vowel-consonant",
                    "from": "vowel",
                    "to": "consonant",
                    "probability": 0.875
                },
                {
                    "id": "consonant-vowel",
                    "from": "consonant",
                    "to": "vowel",
                    "probability": 0.667
                },
                {
                    "id": "consonant-consonant",
                    "from": "consonant",
                    "to": "consonant",
                    "probability": 0.333
                }
            ]
        },
        "explanation": "In 1906, Andrey Markov manually analyzed 20,000 letters from Pushkin's poem 'Eugene Onegin', categorizing them as vowels or consonants. He discovered that the probability of the next letter depended only on the current letter—the Markov property! This simple two-state chain captured the rhythmic structure of Russian poetry and launched an entire field of mathematics.",
        "lessonConnections": [
            {
                "lessonId": "history-1",
                "lessonTitle": "Andrey Markov and the Birth of Memoryless Processes",
                "connection": "This is the exact example Markov used in his 1906 paper! Experience the historical moment when Markov chains were born."
            },
            {
                "lessonId": "chains-1",
                "lessonTitle": "Enter the Markov Chain: Memory-Free Transitions",
                "connection": "This example perfectly demonstrates the Markov property: the next letter depends only on the current letter, not the entire history."
            }
        ],
        "mathematicalDetails": {
            "transitionMatrix": "P = [[0.125, 0.875], [0.667, 0.333]]",
            "stationaryDistribution": "π = [0.432, 0.568] - In the long run, about 43% vowels and 57% consonants",
            "keyInsights": [
                "Markov manually counted 20,000 letters—imagine doing this by hand!",
                "The chain reveals the rhythmic structure of Russian poetry",
                "This was the first real-world application of Markov chains"
            ]
        },
        "realWorldContext": "Markov's analysis showed that even artistic works like poetry follow mathematical laws. Today, similar techniques analyze DNA sequences, music, and language patterns. The Markov property—that the future depends only on the present—is everywhere in nature and art.",
        "practiceQuestions": [
            "If we start with a vowel, what's the probability the next two letters are 'VC'?",
            "What's the stationary distribution? What does it tell us about Pushkin's poetry?",
            "How would you modify this to analyze English poetry instead?"
        ]
    },
    {
        "id": "pagerank",
        "title": "Google PageRank: Ranking the Web",
        "description": "Discover how Google uses Markov chains to rank billions of web pages. The stationary distribution becomes the PageRank score!",
        "category": "modern",
        "difficulty": "intermediate",
        "applications": [
            "Search Engines",
            "Social Network Analysis",
            "Influence Modeling"
        ],
        "interactiveDemo": true,
        "design": {
            "states": [
                {
                    "id": "pageA",
                    "name": "Page A",
                    "x": 700,
                    "y": 500,
                    "color": "#fbbf24"
                },
                {
                    "id": "pageB",
                    "name": "Page B",
                    "x": 1300,
                    "y": 500,
                    "color": "#60a5fa"
                },
                {
                    "id": "pageC",
                    "name": "Page C",
                    "x": 700,
                    "y": 1000,
                    "color": "#34d399"
                },
                {
                    "id": "pageD",
                    "name": "Page D",
                    "x": 1300,
                    "y": 1000,
                    "color": "#a78bfa"
                }
            ],
            "transitions": [
                {
                    "id": "pageA-pageB",
                    "from": "pageA",
                    "to": "pageB",
                    "probability": 0.4625
                },
                {
                    "id": "pageA-pageC",
                    "from": "pageA",
                    "to": "pageC",
                    "probability": 0.4625
                },
                {
                    "id": "pageA-pageA",
                    "from": "pageA",
                    "to": "pageA",
                    "probability": 0.0375
                },
                {
                    "id": "pageA-pageD",
                    "from": "pageA",
                    "to": "pageD",
                    "probability": 0.0375
                },
                {
                    "id": "pageB-pageC",
                    "from": "pageB",
                    "to": "pageC",
                    "probability": 0.8875
                },
                {
                    "id": "pageB-pageA",
                    "from": "pageB",
                    "to": "pageA",
                    "probability": 0.0375
                },
                {
                    "id": "pageB-pageB",
                    "from": "pageB",
                    "to": "pageB",
                    "probability": 0.0375
                },
                {
                    "id": "pageB-pageD",
                    "from": "pageB",
                    "to": "pageD",
                    "probability": 0.0375
                },
                {
                    "id": "pageC-pageA",
                    "from": "pageC",
                    "to": "pageA",
                    "probability": 0.4625
                },
                {
                    "id": "pageC-pageD",
                    "from": "pageC",
                    "to": "pageD",
                    "probability": 0.4625
                },
                {
                    "id": "pageC-pageB",
                    "from": "pageC",
                    "to": "pageB",
                    "probability": 0.0375
                },
                {
                    "id": "pageC-pageC",
                    "from": "pageC",
                    "to": "pageC",
                    "probability": 0.0375
                },
                {
                    "id": "pageD-pageA",
                    "from": "pageD",
                    "to": "pageA",
                    "probability": 0.25
                },
                {
                    "id": "pageD-pageB",
                    "from": "pageD",
                    "to": "pageB",
                    "probability": 0.25
                },
                {
                    "id": "pageD-pageC",
                    "from": "pageD",
                    "to": "pageC",
                    "probability": 0.25
                },
                {
                    "id": "pageD-pageD",
                    "from": "pageD",
                    "to": "pageD",
                    "probability": 0.25
                }
            ]
        },
        "explanation": "PageRank treats the web as a giant Markov chain. A 'random surfer' clicks links (with probability 0.85) or jumps to a random page (with probability 0.15). The stationary distribution—the long-run probability of being on each page—becomes the PageRank score. Pages with high PageRank are ranked higher in search results. This simple idea powers Google's search algorithm!",
        "lessonConnections": [
            {
                "lessonId": "history-3",
                "lessonTitle": "From PageRank to GPT: Markov Chains in the Digital Age",
                "connection": "This is the exact algorithm Larry Page and Sergey Brin described in 1998. The stationary distribution ranks the web!"
            },
            {
                "lessonId": "chains-3",
                "lessonTitle": "Stationary Distributions: The Long-Run Equilibrium",
                "connection": "PageRank is literally the stationary distribution of the web's Markov chain. Understanding stationary distributions is key to understanding search rankings."
            }
        ],
        "mathematicalDetails": {
            "transitionMatrix": "P = α·P_links + (1-α)·(1/N)·1·1^T where α=0.85 (damping factor)",
            "stationaryDistribution": "The PageRank vector π satisfies π = π·P. Pages with high π_i are more important",
            "keyInsights": [
                "Damping factor (0.85) prevents getting stuck in isolated parts of the web",
                "Dangling pages (no outgoing links) are handled by uniform teleportation",
                "The algorithm scales to billions of pages using sparse matrix techniques"
            ]
        },
        "realWorldContext": "PageRank revolutionized web search in 1998. Before PageRank, search engines relied on keyword matching. PageRank introduced the idea that links are 'votes'—a page linked by many important pages must be important itself. Google still uses PageRank (along with hundreds of other signals) today, processing 8.5 billion searches per day.",
        "practiceQuestions": [
            "Which page has the highest PageRank? Why?",
            "What happens to PageRank if we remove the link from C to A?",
            "How does the damping factor affect the ranking? What if α = 0.5 vs α = 0.95?"
        ]
    },
    {
        "id": "text-generation",
        "title": "Text Generation: N-gram Language Model",
        "description": "Generate text using a simple Markov chain that learns word patterns from training data. This is how early chatbots and text generators worked!",
        "category": "modern",
        "difficulty": "intermediate",
        "applications": [
            "Chatbots",
            "Text Generation",
            "Language Modeling"
        ],
        "interactiveDemo": true,
        "design": {
            "states": [
                {
                    "id": "the",
                    "name": "\"the\"",
                    "x": 1000,
                    "y": 350,
                    "color": "#94a3b8"
                },
                {
                    "id": "cat",
                    "name": "\"cat\"",
                    "x": 700,
                    "y": 750,
                    "color": "#fb923c"
                },
                {
                    "id": "sat",
                    "name": "\"sat\"",
                    "x": 1300,
                    "y": 750,
                    "color": "#fbbf24"
                },
                {
                    "id": "on",
                    "name": "\"on\"",
                    "x": 800,
                    "y": 1150,
                    "color": "#60a5fa"
                },
                {
                    "id": "mat",
                    "name": "\"mat\"",
                    "x": 1200,
                    "y": 1150,
                    "color": "#a78bfa"
                }
            ],
            "transitions": [
                {
                    "id": "the-cat",
                    "from": "the",
                    "to": "cat",
                    "probability": 0.4
                },
                {
                    "id": "the-sat",
                    "from": "the",
                    "to": "sat",
                    "probability": 0.1
                },
                {
                    "id": "the-on",
                    "from": "the",
                    "to": "on",
                    "probability": 0.3
                },
                {
                    "id": "the-mat",
                    "from": "the",
                    "to": "mat",
                    "probability": 0.2
                },
                {
                    "id": "cat-sat",
                    "from": "cat",
                    "to": "sat",
                    "probability": 0.6
                },
                {
                    "id": "cat-on",
                    "from": "cat",
                    "to": "on",
                    "probability": 0.2
                },
                {
                    "id": "cat-the",
                    "from": "cat",
                    "to": "the",
                    "probability": 0.2
                },
                {
                    "id": "sat-on",
                    "from": "sat",
                    "to": "on",
                    "probability": 0.7
                },
                {
                    "id": "sat-the",
                    "from": "sat",
                    "to": "the",
                    "probability": 0.3
                },
                {
                    "id": "on-the",
                    "from": "on",
                    "to": "the",
                    "probability": 0.8
                },
                {
                    "id": "on-mat",
                    "from": "on",
                    "to": "mat",
                    "probability": 0.2
                },
                {
                    "id": "mat-the",
                    "from": "mat",
                    "to": "the",
                    "probability": 1.0
                }
            ]
        },
        "explanation": "An n-gram language model is a Markov chain where states are sequences of words. This bigram model (n=2) learns which words follow which words from training text. To generate text, start with initial words and sample the next word from transition probabilities. While simple, this was the foundation of early chatbots and text generators before modern AI!",
        "lessonConnections": [
            {
                "lessonId": "history-3",
                "lessonTitle": "From PageRank to GPT: Markov Chains in the Digital Age",
                "connection": "Early language models were pure n-gram Markov chains. Modern GPT models are sophisticated descendants, but they still capture Markov-like dependencies."
            },
            {
                "lessonId": "chains-1",
                "lessonTitle": "Enter the Markov Chain: Memory-Free Transitions",
                "connection": "This model assumes the next word depends only on the current word—the Markov property. Real language has longer-range dependencies, but this simple model is surprisingly effective."
            }
        ],
        "mathematicalDetails": {
            "transitionMatrix": "P encodes word transition probabilities learned from training text",
            "stationaryDistribution": "π gives long-run word frequencies, reflecting the distribution of words in the training corpus",
            "keyInsights": [
                "First-order Markov models capture local grammatical structure",
                "Training involves counting word pairs in a text corpus",
                "Modern language models use transformers, but n-grams remain foundational"
            ]
        },
        "realWorldContext": "N-gram models were used in early chatbots (2000s), text-to-speech systems, and machine translation. While modern language models (GPT, BERT) use transformers, they still incorporate Markov-like dependencies through attention mechanisms. This simple model demonstrates how probability can generate coherent text!",
        "practiceQuestions": [
            "Starting from 'the', what's the most likely sentence this model would generate?",
            "How would you extend this to a trigram model (n=3)?",
            "What are the limitations of assuming only first-order dependencies in language?"
        ]
    },
    {
        "id": "neutron-chain",
        "title": "Neutron Chain Reaction: Critical Mass",
        "description": "A simplified model of nuclear fission—how neutrons create a chain reaction. This is what von Neumann calculated for the Manhattan Project!",
        "category": "modern",
        "difficulty": "advanced",
        "applications": [
            "Nuclear Physics",
            "Monte Carlo Simulation",
            "Branching Processes"
        ],
        "interactiveDemo": true,
        "design": {
            "states": [
                {
                    "id": "subcritical",
                    "name": "Subcritical",
                    "x": 700,
                    "y": 650,
                    "color": "#34d399"
                },
                {
                    "id": "critical",
                    "name": "Critical",
                    "x": 1000,
                    "y": 650,
                    "color": "#fbbf24"
                },
                {
                    "id": "supercritical",
                    "name": "Supercritical",
                    "x": 1300,
                    "y": 650,
                    "color": "#f87171"
                }
            ],
            "transitions": [
                {
                    "id": "subcritical-subcritical",
                    "from": "subcritical",
                    "to": "subcritical",
                    "probability": 0.7
                },
                {
                    "id": "subcritical-critical",
                    "from": "subcritical",
                    "to": "critical",
                    "probability": 0.25
                },
                {
                    "id": "subcritical-supercritical",
                    "from": "subcritical",
                    "to": "supercritical",
                    "probability": 0.05
                },
                {
                    "id": "critical-critical",
                    "from": "critical",
                    "to": "critical",
                    "probability": 0.5
                },
                {
                    "id": "critical-supercritical",
                    "from": "critical",
                    "to": "supercritical",
                    "probability": 0.3
                },
                {
                    "id": "critical-subcritical",
                    "from": "critical",
                    "to": "subcritical",
                    "probability": 0.2
                },
                {
                    "id": "supercritical-supercritical",
                    "from": "supercritical",
                    "to": "supercritical",
                    "probability": 0.8
                },
                {
                    "id": "supercritical-critical",
                    "from": "supercritical",
                    "to": "critical",
                    "probability": 0.15
                },
                {
                    "id": "supercritical-subcritical",
                    "from": "supercritical",
                    "to": "subcritical",
                    "probability": 0.05
                }
            ]
        },
        "explanation": "In a nuclear reactor, neutrons collide with uranium atoms, causing fission and releasing more neutrons. This creates a branching process—a type of Markov chain. If the average number of neutrons per fission is less than 1, the reaction dies out (subcritical). If it equals 1, the reaction is sustained (critical). If greater than 1, it explodes (supercritical). Von Neumann used Monte Carlo simulation to calculate safe critical masses for the Manhattan Project.",
        "lessonConnections": [
            {
                "lessonId": "history-2",
                "lessonTitle": "Von Neumann, the Manhattan Project, and Critical Mass Calculations",
                "connection": "This is a simplified version of what von Neumann calculated using Monte Carlo methods. The branching process determines whether a nuclear reaction is safe or explosive."
            },
            {
                "lessonId": "chains-3",
                "lessonTitle": "Stationary Distributions: The Long-Run Equilibrium",
                "connection": "For a critical reaction, the stationary distribution tells us the long-run probability of each state—crucial for reactor safety."
            }
        ],
        "mathematicalDetails": {
            "transitionMatrix": "P encodes regime-switching probabilities based on neutron multiplication factor",
            "stationaryDistribution": "For critical reactions, π reflects the balance between neutron production and loss",
            "keyInsights": [
                "This is a branching process—each neutron can produce multiple 'offspring'",
                "Criticality depends on the mean number of neutrons per fission",
                "Monte Carlo simulation was essential because analytical solutions were impossible"
            ]
        },
        "realWorldContext": "Von Neumann's calculations were crucial for safety—preventing accidental explosions during experiments and enabling safe nuclear power development. Today, similar Monte Carlo methods simulate particle physics at CERN, model climate change, and price financial derivatives. The Manhattan Project gave birth to Monte Carlo simulation, now one of the most powerful tools in science and engineering.",
        "practiceQuestions": [
            "What's the stationary distribution? Which state dominates in the long run?",
            "If you're in a critical state, what's the probability of reaching supercritical in 2 steps?",
            "How would you modify this model to include control rods that absorb neutrons?"
        ]
    },
    {
        "id": "queue-system",
        "title": "Queueing System: Waiting in Line",
        "description": "Model a simple queue where customers arrive and get served. Understand waiting times, server utilization, and system stability—essential for call centers, restaurants, and traffic!",
        "category": "classic",
        "difficulty": "intermediate",
        "applications": [
            "Call Centers",
            "Restaurant Management",
            "Traffic Flow"
        ],
        "interactiveDemo": true,
        "design": {
            "states": [
                {
                    "id": "q0",
                    "name": "0 customers",
                    "x": 1000,
                    "y": 300,
                    "color": "#34d399"
                },
                {
                    "id": "q1",
                    "name": "1 customer",
                    "x": 1000,
                    "y": 540,
                    "color": "#60a5fa"
                },
                {
                    "id": "q2",
                    "name": "2 customers",
                    "x": 1000,
                    "y": 780,
                    "color": "#fbbf24"
                },
                {
                    "id": "q3",
                    "name": "3 customers",
                    "x": 1000,
                    "y": 1020,
                    "color": "#fb923c"
                },
                {
                    "id": "q4plus",
                    "name": "4+ customers",
                    "x": 1000,
                    "y": 1260,
                    "color": "#f87171"
                }
            ],
            "transitions": [
                {
                    "id": "q0-q1",
                    "from": "q0",
                    "to": "q1",
                    "probability": 0.6
                },
                {
                    "id": "q0-q0",
                    "from": "q0",
                    "to": "q0",
                    "probability": 0.4
                },
                {
                    "id": "q1-q2",
                    "from": "q1",
                    "to": "q2",
                    "probability": 0.6
                },
                {
                    "id": "q1-q0",
                    "from": "q1",
                    "to": "q0",
                    "probability": 0.3
                },
                {
                    "id": "q1-q1",
                    "from": "q1",
                    "to": "q1",
                    "probability": 0.1
                },
                {
                    "id": "q2-q3",
                    "from": "q2",
                    "to": "q3",
                    "probability": 0.6
                },
                {
                    "id": "q2-q1",
                    "from": "q2",
                    "to": "q1",
                    "probability": 0.3
                },
                {
                    "id": "q2-q2",
                    "from": "q2",
                    "to": "q2",
                    "probability": 0.1
                },
                {
                    "id": "q3-q4plus",
                    "from": "q3",
                    "to": "q4plus",
                    "probability": 0.6
                },
                {
                    "id": "q3-q2",
                    "from": "q3",
                    "to": "q2",
                    "probability": 0.3
                },
                {
                    "id": "q3-q3",
                    "from": "q3",
                    "to": "q3",
                    "probability": 0.1
                },
                {
                    "id": "q4plus-q4plus",
                    "from": "q4plus",
                    "to": "q4plus",
                    "probability": 0.7
                },
                {
                    "id": "q4plus-q3",
                    "from": "q4plus",
                    "to": "q3",
                    "probability": 0.3
                }
            ]
        },
        "explanation": "This birth-death queue models a system where customers arrive (increasing queue length) and get served (decreasing queue length). From states 0-3, arrivals occur with probability 0.6, services with 0.3, and otherwise the state stays the same (0.1). The '4+' state aggregates all larger queues. The stationary distribution tells you the long-run probability of different queue lengths—essential for capacity planning!",
        "lessonConnections": [
            {
                "lessonId": "ctmc-2",
                "lessonTitle": "Queueing Systems: When Waiting Becomes Mathematics",
                "connection": "This discrete-time queue is a simplified version of continuous-time queueing systems. The same principles apply: arrivals increase queue length, services decrease it."
            },
            {
                "lessonId": "chains-3",
                "lessonTitle": "Stationary Distributions: The Long-Run Equilibrium",
                "connection": "The stationary distribution reveals the long-run probability of having 0, 1, 2, 3, or 4+ customers. This is crucial for capacity planning—if π(4+) is high, you need more servers!"
            }
        ],
        "mathematicalDetails": {
            "transitionMatrix": "P is a birth-death process: transitions only occur to neighboring states",
            "stationaryDistribution": "For stable queues (arrival rate < service rate), π exists and can be computed recursively",
            "keyInsights": [
                "This is a discrete-time approximation of continuous-time queueing systems",
                "The '4+' state aggregates all larger queues—a common modeling technique",
                "Stability requires that the arrival rate is less than the service rate on average"
            ]
        },
        "realWorldContext": "Queueing models are everywhere: call centers (customers = callers, servers = operators), restaurants (customers = diners, servers = tables), internet routers (customers = packets, servers = transmission capacity). Understanding queue behavior helps design efficient systems and predict wait times. Next time you're waiting in line, you're experiencing a Markov chain in action!",
        "practiceQuestions": [
            "What's the probability the queue is empty in steady state?",
            "If arrivals increase to 0.7, what happens to the stationary distribution?",
            "How would adding a second server change the transition probabilities?"
        ]
    }
]
